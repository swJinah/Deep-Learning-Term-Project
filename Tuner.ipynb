{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tuner.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMix6WFDEFqFbV2bypoPTwe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bQrht-CuizZz"},"source":["# import module\n","from sklearn.model_selection import train_test_split\n","import torchvision.models as models\n","import torch.optim as optim\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import logging.handlers\n","import datetime\n","import argparse\n","import logging.handlers\n","\n","class TuneParams():\n","\n","    def __init__(self):\n","        self.parser = argparse.ArgumentParser(description='Tune Parameters...')\n","\n","        # data\n","        self.parser.add_argument('--batch_size', default=32, type=int, help='number of batch size')\n","\n","        # test rate\n","        self.parser.add_argument('--rate', default=0.1, type=float, help='split rate(default : 0.1)')\n","\n","        # epoch\n","        self.parser.add_argument('--epochs', type=int, default=5, help='number of total training epochs')\n","\n","        # learning rate\n","        self.parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n","                                 help='learning rate(deafult : 0.1)')\n","\n","        # optimizer & scheduler(various choices)\n","        self.parser.add_argument('--optim', type=str, default='sgd', choices=['sgd', 'adam'])\n","        self.parser.add_argument('--lr_scheduler', default='none', type=str, choices=['multistep','stepLR','LambdaLR', 'none'])\n","\n","        self.args = self.parser.parse_args(args=[])\n","\n","    def create_optim(self, params):\n","        if self.args.optim == 'sgd':\n","            optimizer = optim.SGD(params, lr=self.args.lr)\n","        elif self.args.optim == 'adam':\n","            optimizer = optim.Adam(params, lr=self.args.lr, weight_decay=0.0)\n","\n","        return optimizer\n","\n","    def create_scheduler(self, optimizer):\n","        if self.args.lr_scheduler == 'multistep':\n","            scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,35,45], gamma=0.1)\n","        elif self.args.lr_scheduler == 'stepLR':\n","            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","        elif self.args.lr_scheduler == 'LambdaLR':\n","            scheduler= optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n","                                                    lr_lambda=lambda epoch: 0.95 ** epoch,\n","                                                    last_epoch=-1,\n","                                                    verbose=False)\n","        elif self.args.lr_scheduler == 'none':\n","            scheduler = None\n","\n","        else:\n","            raise ValueError(\"No Scheduler\\n\")\n","\n","        return scheduler\n","    \n","    def custom_input(self,lr_list=[0.1],optim_list=['sgd'],scheduler_list=['multistep'],test_ratio_list=[0.1]):\n","        # result stores accuracy, args, index(iterate each learning rate, optimizer, scheduler)\n","        # ex) 59.8333, \"Namespace(batch_size=32, epochs=10, lr=0.1, lr_scheduler='multistep', optim='sgd')\" ...\n","        result = []\n","        i = 1  # index stores the best model's index\n","\n","        # ========== set logger ==========\n","        logger = logging.getLogger('logger')\n","        fomatter = logging.Formatter('[%(levelname)s| %(filename)s:%(lineno)s] >> %(message)s')\n","        time = datetime.datetime.now().strftime(\"%m%d_%H:%M\")\n","        fileHandler = logging.FileHandler('./log_' + time + '.txt')\n","        fileHandler.setFormatter(fomatter)\n","        logger.addHandler(fileHandler)\n","        logger.setLevel(logging.DEBUG)\n","        # ================================\n","\n","        # execute possibe combinations\n","        # to get highest performance combination and that is the best params\n","        for rate in range(len(test_ratio_list)):\n","            for a in range(len(lr_list)):\n","                for b in range(len(optim_list)):\n","                    for c in range(len(scheduler_list)):\n","                        self.args.rate = test_ratio_list[rate]\n","                        self.args.lr = lr_list[a]\n","                        self.args.optim = optim_list[b]\n","                        self.args.lr_scheduler = scheduler_list[c]\n","\n","                        print(self.args)\n","\n","                        # info about possible combinations\n","                        logger.info(\"========================================\")\n","                        logger.info(\n","                            \"batch_size: {}, total epoch: {}, test data ratio: {}, initial learning rate: {}, optimizer: {}, scheduler: {} \"\n","                                .format(self.args.batch_size, self.args.epochs, self.args.rate, self.args.lr,\n","                                        self.args.optim, self.args.lr_scheduler))\n","\n","                        argString = str(self.args)\n","                        accuracy = testing(self, i, logger)\n","                        result.append((accuracy, argString, i))\n","                        i = i + 1\n","\n","        print('Before sorting')\n","        print(result)\n","        result.sort(key=lambda x: x[0], reverse=True)\n","\n","        print('After sorting')\n","        print(result)\n","        print(result[0][1])\n","\n","        model_pth = \"./best_model_[\" + str(result[0][2]) + \"].pth\"  # ex) best_model_[8].pth\n","        print(model_pth)\n"],"execution_count":null,"outputs":[]}]}